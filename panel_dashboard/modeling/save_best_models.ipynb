{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78303d37-a284-4118-9930-fd44d34d01ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 09:10:46.808972: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score \n",
    "\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279e43f-061c-4ae4-8c9f-bfd6b1c9604e",
   "metadata": {},
   "source": [
    "# fit and save best model for all portfolio classes based upon final evaluation. Evaluations can be found within each models individual notebook and within the 'metric_comparisons' notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb064e-95b4-4104-9e09-466bd6041eb1",
   "metadata": {},
   "source": [
    "### All portfolio classes found that Random Forest to provide the best results, except for the balanced class which found the neural network to provide the best results.\n",
    "### Random Forest had the optimal performance with the full feature set while the neural network worked best with the reduced feature set\n",
    "\n",
    "#### loading train/test data for reduced or full features depending on model being used and defining Random Forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ca7c3c-d393-4e88-a36c-5433f33cd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # load X_train_reduced and X_test_reduced\n",
    "X_train_full_conservative = pd.read_csv(Path(\"./data/X_train_full_conservative.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True)\n",
    "X_test_full_conservative = pd.read_csv(Path(\"./data/X_test_full_conservative.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "X_train_reduced_balanced = pd.read_csv(Path(\"./data/X_train_reduced_balanced.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True)\n",
    "X_test_reduced_balanced = pd.read_csv(Path(\"./data/X_test_reduced_balanced.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "X_train_full_growth = pd.read_csv(Path(\"./data/X_train_full_growth.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True)\n",
    "X_test_full_growth = pd.read_csv(Path(\"./data/X_test_full_growth.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "X_train_full_aggressive = pd.read_csv(Path(\"./data/X_train_full_aggressive.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True)\n",
    "X_test_full_aggressive = pd.read_csv(Path(\"./data/X_test_full_aggressive.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "X_train_full_alternative = pd.read_csv(Path(\"./data/X_train_full_alternative.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True)\n",
    "X_test_full_alternative = pd.read_csv(Path(\"./data/X_test_full_alternative.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "#load y_train and y_test\n",
    "y_train_conservative = pd.read_csv(Path(\"./data/y_train_conservative.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True).values.ravel()\n",
    "y_test_conservative = pd.read_csv(Path(\"./data/y_test_conservative.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True).values.ravel()\n",
    "\n",
    "y_train_balanced = pd.read_csv(Path(\"./data/y_train_balanced.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True).values.ravel()\n",
    "y_test_balanced = pd.read_csv(Path(\"./data/y_test_balanced.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True).values.ravel()\n",
    "\n",
    "y_train_growth = pd.read_csv(Path(\"./data/y_train_growth.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True).values.ravel()\n",
    "y_test_growth = pd.read_csv(Path(\"./data/y_test_growth.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True).values.ravel()\n",
    "\n",
    "y_train_aggressive = pd.read_csv(Path(\"./data/y_train_aggressive.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True).values.ravel()\n",
    "y_test_aggressive = pd.read_csv(Path(\"./data/y_test_aggressive.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True).values.ravel()\n",
    "\n",
    "y_train_alternative = pd.read_csv(Path(\"./data/y_train_alternative.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True).values.ravel()\n",
    "y_test_alternative = pd.read_csv(Path(\"./data/y_test_alternative.csv\"), index_col=\"Unnamed: 0\", parse_dates=True, infer_datetime_format=True).values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51b32ea-6f02-4e34-97e4-b674729376a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=1000,\n",
    "                                  max_depth=40,\n",
    "                                  min_samples_split=50,\n",
    "                                  min_samples_leaf=20,\n",
    "                                  max_features=None,\n",
    "                                  bootstrap=True,\n",
    "                                  criterion='entropy', \n",
    "                                  min_impurity_decrease=0.01,\n",
    "                                  class_weight={0: 1, 1: 5},\n",
    "                                  oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df921d9f-b47d-4ac4-abd1-b07296394aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9394f364-67cc-45c4-95a1-e27c1a5cc0f3",
   "metadata": {},
   "source": [
    "#### Define standard scaler to use in model pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629c10e6-b6e5-492b-a662-2e8bc234a200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867b142-ad01-4f13-9f3b-a1368533d13d",
   "metadata": {},
   "source": [
    "## training best model for each portfolio class and saving model for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6941dcd-14c1-429c-8f46-b87623af3ffa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aggressive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb32f94-f2a2-4360-b265-00fd28071ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8171)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('scaler', scaler), ('model', rf_model)])\n",
    "pipeline.fit(X_train_full_aggressive, y_train_aggressive)\n",
    "\n",
    "filepath = Path(\"./saved_models/aggressive.joblib\")\n",
    "with open(filepath, 'wb') as file:\n",
    "    dump(pipeline, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073aa80c-bc18-4da1-b380-e05875c60d39",
   "metadata": {},
   "source": [
    "### Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89006c17-967f-4b04-ad35-c13869c07af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8171)\n",
    "\n",
    "pipeline = Pipeline([('scaler', scaler), ('model', rf_model)])\n",
    "pipeline.fit(X_train_full_alternative, y_train_alternative)\n",
    "\n",
    "filepath = Path(\"./saved_models/alternative.joblib\")\n",
    "with open(filepath, 'wb') as file:\n",
    "    dump(pipeline,file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a7fd0-cc01-4f9e-8fc5-d88e23fa2ee9",
   "metadata": {},
   "source": [
    "### Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb030403-9b7f-4793-93e9-94cf1137ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 09:10:56.242805: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# pipeline = Pipeline([('scaler', scaler), ('model', rf_model)])\n",
    "# pipeline.fit(X_train_full_balanced, y_train_balanced)\n",
    "# dump(pipeline, Path(\"./saved_models/balanced.joblib\"))\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "    \n",
    "   # Create the scaler instance\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler.fit(X_train_reduced_balanced)\n",
    "\n",
    "# Scale the data\n",
    "X_train_reduced_balanced_scaled = X_scaler.transform(X_train_reduced_balanced)\n",
    "X_test_reduced_balanced_scaled = X_scaler.transform(X_test_reduced_balanced)\n",
    "number_input_features = 7\n",
    "hidden_nodes_layer1 = 32\n",
    "hidden_nodes_layer2 = 3\n",
    "activation_1 = 'tanh'\n",
    "activation_2 = 'tanh'\n",
    "lr = 0.01\n",
    "\n",
    "# Create a sequential neural network model\n",
    "nn_balanced = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_balanced.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=activation_1))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_balanced.add(Dense(units=hidden_nodes_layer2, activation=activation_2))\n",
    "\n",
    "# Add the output layer\n",
    "nn_balanced.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model \n",
    "# Set the parameters as mean_squared_error, adam, and accuracy.\n",
    "nn_balanced.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "deep_net_balanced_model = nn_balanced.fit(X_train_reduced_balanced_scaled, y_train_balanced, epochs=100, verbose=0)\n",
    "\n",
    "\n",
    "nn_balanced.save(Path(\"./saved_models/balanced.h5\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed20c5-c0a9-447d-9b67-0d62079e6885",
   "metadata": {},
   "source": [
    "### Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f4472ff-c4d9-4a42-8245-61bb1822829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8171)\n",
    "\n",
    "pipeline = Pipeline([('scaler', scaler), ('model', rf_model)])\n",
    "pipeline.fit(X_train_full_conservative, y_train_conservative)\n",
    "\n",
    "filepath = Path(\"./saved_models/conservative.joblib\")\n",
    "with open(filepath, 'wb') as file:\n",
    "    dump(pipeline, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b38c4c0-2109-4b92-a3b9-1deaa3e95702",
   "metadata": {},
   "source": [
    "### Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d5e7a2-055d-4193-9389-cfa17953113a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(8171)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('scaler', scaler), ('model', rf_model)])\n",
    "pipeline.fit(X_train_full_growth, y_train_growth)\n",
    "\n",
    "filepath = Path(\"./saved_models/growth.joblib\")\n",
    "with open(filepath, 'wb') as file:\n",
    "    dump(pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09c8e4-480a-4304-a7d6-3d141dbecfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
